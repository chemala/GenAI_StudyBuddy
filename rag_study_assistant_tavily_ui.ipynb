{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chemala/GenAI_StudyBuddy/blob/dev/rag_study_assistant_tavily_ui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ooW2jdWywOl-"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber faiss-cpu sentence-transformers tavily-python gradio langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSXsqrKqwaws"
      },
      "outputs": [],
      "source": [
        "import os, io\n",
        "import pdfplumber\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tavily import TavilyClient\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsIoSVD6wn2w"
      },
      "outputs": [],
      "source": [
        "TAVILY_API_KEY = \"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\" #1000 Free Credits / Month\n",
        "\n",
        "print(\"Tavily key set:\", bool(TAVILY_API_KEY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmbbRCvNxE_k"
      },
      "source": [
        "Extract text from PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdKXSt2OwrjJ"
      },
      "outputs": [],
      "source": [
        "# def extract_pdf_text(pdf_bytes):\n",
        "#     text = \"\"\n",
        "#     with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:\n",
        "#         for page in pdf.pages:\n",
        "#             t = page.extract_text()\n",
        "#             if t:\n",
        "#                 text += t + \"\\n\"\n",
        "#     return text\n",
        "\n",
        "def extract_pdf_text_from_path(path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            t = page.extract_text()\n",
        "            if t:\n",
        "                text += t + \"\\n\"\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDUdwwiVxCG9"
      },
      "source": [
        "Chunk text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOJnnt11wuur"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, chunk_size=600, overlap=120):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    length = len(text)\n",
        "\n",
        "    while start < length:\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start = end - overlap\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uww1iyvcw6yJ"
      },
      "source": [
        "Make embeddings from text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vztuk3NmwxQR"
      },
      "outputs": [],
      "source": [
        "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "EMB_DIM = embedder.get_sentence_embedding_dimension()\n",
        "\n",
        "def embed(texts):\n",
        "    return embedder.encode(texts, convert_to_numpy=True).astype(\"float32\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng_G9n9hw5cZ"
      },
      "source": [
        "Build FAISS Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-2URlRBw3xm"
      },
      "outputs": [],
      "source": [
        "def build_index(chunks):\n",
        "    vectors = embed(chunks)\n",
        "    index = faiss.IndexFlatL2(EMB_DIM)\n",
        "    index.add(vectors)\n",
        "    return index, chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZKF1dToYFCK"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_JIkpqHXSH0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    use_fast=True\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    dtype=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=1000,\n",
        "    do_sample=True,\n",
        "    temperature=0.3\n",
        ")"
      ],
      "metadata": {
        "id": "8yjXpsLZfwYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIaRb7VVxIo1"
      },
      "source": [
        "Use Tavily Web Access Layer for web search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GgcNKZuxH1r"
      },
      "outputs": [],
      "source": [
        "def tavily_search(query, api_key, k=3):\n",
        "    client = TavilyClient(api_key=api_key)\n",
        "    res = client.search(query, limit=k)\n",
        "    return [r.get(\"content\", \"\") for r in res['results']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kO0V5PEPo-Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX9k2iUoxYfX"
      },
      "source": [
        "Retrieve Local Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7EEFoV1xYqy"
      },
      "outputs": [],
      "source": [
        "def retrieve_local(query, index, chunks, k=5):\n",
        "    qv = embed([query])\n",
        "    _, I = index.search(qv, k)\n",
        "    return [chunks[i] for i in I[0]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePXMuXiOxfTT"
      },
      "source": [
        " RAG (local/web/hybrid depending on parameters) - we can just make three separate options to select on of the modes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A0IRCydxmuo"
      },
      "outputs": [],
      "source": [
        "def rag(query, mode, index, chunks, api_key):\n",
        "    context = []\n",
        "\n",
        "    if mode in [\"local\", \"hybrid\"]:\n",
        "        context += retrieve_local(query, index, chunks)\n",
        "\n",
        "    if mode in [\"web\", \"hybrid\"] and api_key:\n",
        "        context += tavily_search(query, api_key)\n",
        "\n",
        "    return \"\\n\\n---\\n\\n\".join(context)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W_5KB8I7B2o"
      },
      "outputs": [],
      "source": [
        "state = {\n",
        "    \"index\": None,\n",
        "    \"chunks\": None\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFKROv2e7n_X"
      },
      "outputs": [],
      "source": [
        "def build(files):\n",
        "    if not files:\n",
        "        return \"❌ No files uploaded.\"\n",
        "\n",
        "    texts = []\n",
        "    for f in files:\n",
        "        text = extract_pdf_text_from_path(f)\n",
        "        texts += chunk_text(text)\n",
        "\n",
        "    idx, ch = build_index(texts)\n",
        "    state[\"index\"] = idx\n",
        "    state[\"chunks\"] = ch\n",
        "\n",
        "    return f\"✅ Indexed {len(ch)} chunks.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MIh7ieaaPff"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedding_model = embedder # Use the same embedder that built the index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49dacf6c"
      },
      "source": [
        "import json\n",
        "\n",
        "def rag_with_llm(question, mode, index, chunks, tavily_api_key, chat_history, top_k=5):\n",
        "    # 1✅ Embed question\n",
        "    q_emb = embedding_model.encode(\n",
        "        [question],\n",
        "        normalize_embeddings=True\n",
        "    ).astype(\"float32\")\n",
        "\n",
        "    # 2✅ Retrieve from FAISS\n",
        "    D, I = index.search(q_emb, top_k)\n",
        "    local_context = \"\\n\".join(chunks[i] for i in I[0])\n",
        "\n",
        "    # 3✅ Optional web search\n",
        "    web_context = \"\"\n",
        "    if mode in [\"web\", \"hybrid\"]:\n",
        "        tavily = TavilyClient(api_key=tavily_api_key)\n",
        "        web = tavily.search(question, max_results=3)\n",
        "        web_context = \"\\n\".join(r[\"content\"] for r in web['results'])\n",
        "\n",
        "    # 4✅ Build prompt\n",
        "    context = local_context\n",
        "    if web_context:\n",
        "        context += \"\\n\\nWeb context:\\n\" + web_context\n",
        "\n",
        "    # Include chat history in the prompt\n",
        "    history_str = \"\"\n",
        "    if chat_history:\n",
        "        history_str = \"\\n\\nPrevious conversation:\\n\" + \"\\n\".join([f\"User: {h['question']}\\nAssistant: {h['answer']}\" for h in chat_history])\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]\n",
        "Based *strictly* on the following context and previous conversation, formulate a helpful response to the query. Provide information or tips *only as directly relevant* to the question and found within the context. Do not ask clarifying questions, introduce new topics, or discuss linguistic nuances unless explicitly asked about them in the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "{history_str}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "    # 5✅ Generate answer\n",
        "    result = llm(prompt)\n",
        "    answer = result[0][\"generated_text\"].split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    # Update chat history\n",
        "    new_chat_history = chat_history + [{'question': question, 'answer': answer}]\n",
        "\n",
        "    return answer, new_chat_history\n",
        "\n",
        "def ask(q, mode, api_key):\n",
        "    if state[\"index\"] is None:\n",
        "        # Return an empty string for the question to keep it as is, not clear it on error\n",
        "        return \"❌ Build the index first.\", [], gr.update(value=q)\n",
        "\n",
        "    chat_history = state[\"chat_history\"]\n",
        "\n",
        "    answer, updated_chat_history = rag_with_llm(\n",
        "        q,\n",
        "        mode,\n",
        "        state[\"index\"],\n",
        "        state[\"chunks\"],\n",
        "        api_key,\n",
        "        chat_history\n",
        "    )\n",
        "\n",
        "    state[\"chat_history\"] = updated_chat_history\n",
        "\n",
        "    # Format chat history for Gradio Chatbot\n",
        "    formatted_history = [[h['question'], h['answer']] for h in state['chat_history']]\n",
        "\n",
        "    # Return the answer, formatted history, and a command to clear the question textbox\n",
        "    return answer, formatted_history, gr.update(value=\"\")\n",
        "\n",
        "def generate_flashcards(flashcard_topic, num_flashcards, mode, index, chunks, api_key):\n",
        "    # 2. Construct a query string\n",
        "    query = f\"Information about {flashcard_topic}\"\n",
        "\n",
        "    # 3. Call the rag function to obtain relevant context\n",
        "    context = rag(query, mode, index, chunks, api_key)\n",
        "\n",
        "    # 4. Craft a detailed prompt for the LLM\n",
        "    prompt = f\"\"\"<s>[INST]\n",
        "Based on the following context, generate {num_flashcards} flashcards about '{flashcard_topic}'.\n",
        "Each flashcard should have a 'question' and an 'answer'.\n",
        "Return the output as a JSON array of objects, where each object has a 'question' key and an 'answer' key.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Example format:\n",
        "[\n",
        "  {{\"question\": \"What is X?\", \"answer\": \"Y.\"}},\n",
        "  {{\"question\": \"What is A?\", \"answer\": \"B.\"}}\n",
        "]\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "    # 5. Call the llm pipeline with this prompt\n",
        "    llm_response = llm(prompt)\n",
        "    raw_response = llm_response[0][\"generated_text\"].split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    # 6. Parse the LLM's response\n",
        "    try:\n",
        "        # Attempt to find the JSON part of the response, as LLMs can sometimes add extra text\n",
        "        json_start = raw_response.find('[')\n",
        "        json_end = raw_response.rfind(']') + 1\n",
        "        if json_start != -1 and json_end != -1 and json_end > json_start:\n",
        "            json_string = raw_response[json_start:json_end]\n",
        "            flashcards = json.loads(json_string)\n",
        "        else:\n",
        "            # If JSON delimiters not found, try to parse the whole string\n",
        "            flashcards = json.loads(raw_response)\n",
        "        return flashcards\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error parsing JSON response: {e}\")\n",
        "        print(f\"Raw LLM response: {raw_response}\")\n",
        "        return [] # Return empty list on parsing error\n",
        "\n",
        "def call_generate_flashcards(flashcard_topic, num_flashcards, mode, api_key):\n",
        "    if state[\"index\"] is None:\n",
        "        return \"❌ Build the index first before generating flashcards.\", []\n",
        "\n",
        "    flashcards_data = generate_flashcards(\n",
        "        flashcard_topic,\n",
        "        int(num_flashcards),\n",
        "        mode,\n",
        "        state[\"index\"],\n",
        "        state[\"chunks\"],\n",
        "        api_key\n",
        "    )\n",
        "\n",
        "    # Format for gr.DataFrame\n",
        "    formatted_flashcards = [[card['question'], card['answer']] for card in flashcards_data]\n",
        "    return formatted_flashcards\n",
        "\n",
        "def launch_ui():\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"## ✊ RAG Study Assistant\")\n",
        "\n",
        "        files = gr.File(\n",
        "            file_types=[\".pdf\"],\n",
        "            file_count=\"multiple\",\n",
        "            label=\"Upload PDFs\"\n",
        "        )\n",
        "\n",
        "        api = gr.Textbox(\n",
        "            label=\"Tavily API Key\",\n",
        "            value=TAVILY_API_KEY,\n",
        "            type=\"password\"\n",
        "        )\n",
        "\n",
        "        build_btn = gr.Button(\"Build Index\")\n",
        "        status = gr.Textbox(label=\"Status\")\n",
        "\n",
        "        mode = gr.Radio(\n",
        "            [\"local\", \"web\", \"hybrid\"],\n",
        "            value=\"hybrid\",\n",
        "            label=\"Search mode\"\n",
        "        )\n",
        "\n",
        "        question = gr.Textbox(label=\"Question\", lines=2)\n",
        "\n",
        "        # Add Chatbot component for displaying history\n",
        "        chatbot = gr.Chatbot(\n",
        "            label=\"Chat History\",\n",
        "            height=300 # Set a fixed height for the chatbot window\n",
        "        )\n",
        "\n",
        "        answer = gr.Textbox(label=\"Answer\", lines=12)\n",
        "\n",
        "        # New components for flashcard generation\n",
        "        flashcard_topic = gr.Textbox(\n",
        "            label=\"Flashcard Topic\",\n",
        "            placeholder=\"e.g., Key concepts from the document\"\n",
        "        )\n",
        "        num_flashcards = gr.Number(\n",
        "            label=\"Number of Flashcards\",\n",
        "            value=5,\n",
        "            minimum=1,\n",
        "            maximum=20,\n",
        "            step=1\n",
        "        )\n",
        "\n",
        "        # Add gr.DataFrame for displaying generated flashcards\n",
        "        flashcard_display = gr.DataFrame(\n",
        "            headers=[\"Question\", \"Answer\"],\n",
        "            value=[],\n",
        "            label=\"Generated Flashcards\"\n",
        "        )\n",
        "\n",
        "        ask_btn = gr.Button(\"Ask\")\n",
        "        generate_flashcards_btn = gr.Button(\"Generate Flashcards\") # New button\n",
        "\n",
        "        build_btn.click(build, inputs=files, outputs=status)\n",
        "\n",
        "        # Update ask_btn.click to return the answer, chat history, and clear the question textbox\n",
        "        ask_btn.click(\n",
        "            fn=ask,\n",
        "            inputs=[question, mode, api],\n",
        "            outputs=[answer, chatbot, question] # Now outputs both the answer, chat history, and the question component for clearing\n",
        "        )\n",
        "\n",
        "        # Attach the call_generate_flashcards function to the new button's click event\n",
        "        generate_flashcards_btn.click(\n",
        "            fn=call_generate_flashcards,\n",
        "            inputs=[flashcard_topic, num_flashcards, mode, api],\n",
        "            outputs=flashcard_display\n",
        "        )\n",
        "\n",
        "        # Function to update the chatbot display from the state's chat_history\n",
        "        def update_chatbot_display():\n",
        "            return [[h['question'], h['answer']] for h in state['chat_history']]\n",
        "\n",
        "        # Initial load and subsequent updates for the chatbot\n",
        "        demo.load(update_chatbot_display, inputs=None, outputs=chatbot)\n",
        "        # Removed the problematic line with _js\n",
        "\n",
        "    demo.launch(share=True, debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "launch_ui()"
      ],
      "metadata": {
        "id": "wzu7X7Jvn-xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad7efe42"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the newly implemented flashcard generation feature, explaining how users can now specify a topic and quantity to create flashcards from their study material and web context, and how these are displayed in the Gradio interface.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ca45d63"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **User Interface for Flashcard Generation**: The Gradio user interface was enhanced with two new input components:\n",
        "    *   A `gr.Textbox` labeled \"Flashcard Topic\" to allow users to specify the subject for flashcard creation.\n",
        "    *   A `gr.Number` labeled \"Number of Flashcards\" which enables users to set the desired quantity of flashcards, with a default of 5, a minimum of 1, and a maximum of 20.\n",
        "*   **Flashcard Generation Logic**: A new Python function, `generate_flashcards`, was developed to create flashcards. This function:\n",
        "    *   Constructs a query based on the user-provided `flashcard_topic`.\n",
        "    *   Retrieves relevant context using a Retrieval Augmented Generation (RAG) approach, drawing from both local study materials (through an index and chunks) and optional web searches.\n",
        "    *   Sends a carefully crafted prompt to a Large Language Model (LLM) to generate flashcards in a JSON array format, each with \"question\" and \"answer\" keys.\n",
        "    *   Includes robust JSON parsing to extract the flashcards from the LLM's raw output, handling cases where the LLM might include additional text.\n",
        "*   **Flashcard Display Component**: A `gr.DataFrame` component was added to the Gradio UI, labeled \"Generated Flashcards,\" configured with \"Question\" and \"Answer\" headers, to visually present the output from the `generate_flashcards` function.\n",
        "*   **Integrated Flashcard Workflow**: A new \"Generate Flashcards\" button was added to the UI. When this button is clicked, it invokes an intermediary function, `call_generate_flashcards`. This function gathers the user-specified topic, quantity, search mode, and API key, calls the `generate_flashcards` logic, and then populates the `gr.DataFrame` (`flashcard_display`) with the generated flashcards. An initial check ensures that the RAG index has been built before attempting flashcard generation.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The newly implemented flashcard generation feature provides users with a powerful tool to create study aids dynamically from their materials and web context, greatly enhancing the study assistant's utility. Users can easily specify their desired topic and quantity, and the system intelligently leverages RAG and LLMs to produce structured flashcards displayed directly within the Gradio interface.\n",
        "*   Future development could focus on adding functionalities such as saving or exporting the generated flashcards, allowing users to edit flashcard content, or integrating a basic spaced repetition system for active recall.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}